## Training NN models
-TODO

## Building dictionaries
-TODO

## Vectorizing data
-Read Finnish and English parsebank files, and filter data to keep only unique sentences which passes lenth and character filters, split the senteces into size-wise batches:
python3 filter_uniq.py --max_pairs 0

-Read finnish and english text file and turn these into dense vectors:
for n in {5..30} ; do echo $n ; THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32,cnmem=0 python vectorize_dense.py -m ep100k_models/ep100k.conv.37 -v ep100k_models/ep100k.conv-vocab.pickle -l $n --max_pairs 0 ; done

## Cluster dense vectors
-Calculate cluster centers based on a sample of the data (produces dir/lang.cluster.centers.pkl):
time python cluster_dense.py --dir vdata_ep100k --lang en --ratio 0.05 --clusters 1000 (~48 min evex)

-Distribute data to cluster based files:
time python data_to_clusters.py --dir vdata_ep100k --lang en --files 100 --clusters 1000 (~443min evex)
time python data_to_clusters.py --dir vdata_ep100k --lang fi --files 100 --clusters 1000 (~246min evex)

## Run dot product
-Calculate sparse matrices, and run dense and sparse dot products inside clusters, slice data according to cluster ids and sentence lengths to keep things fast:
time python dot.py --fi_fname vdata_ep100k/clusters/fi.clusters430-439 --en_fname vdata_ep100k/clusters/en.clusters430-439 --dictionary europarl100K_exp/ep100k.lex --dict_vocabulary europarl100K_exp/uniq.train.tokens.100K --outfile results_ep100k/clusters430-439.results > clusters430-439.log 2>&1 (clusters430-439 (smallest) = 23 min, clusters10-19 (biggest) = 233 min)
    
